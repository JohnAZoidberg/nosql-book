\chapter{Apache Cassandra}
\chapterauthor{David Marchi, Daniel Sch√§fer, Erik Zeiske}

\section{Introduction}
%- History, relevance, environment, context
%- Task, goal, why, what
%- Structure
Scope: TODO \\
Cassandra can be made aware of the relative physical location of all nodes, for example in which rack or datacenter they are. For real world usage it is recommended to do that and also adjust the configuration to take that into account. For trying out Cassandra on a single node or a small local cluster however all of those settings can be left at their defaults. To keep this chapter at a manageable size we will not explain the topological features, only mention that they are available.

\subsection{Abstract}
\subsection{Overview of Cassandra}
Cassandra is a Wide Column Store Database. It is table-like but not relational. The architecture and design make it a distributed and masterless data store for large data. It is written in Java and can be run distributed on commodity hardware.
Since it is masterless there is no single point of failure. Nodes (connected instances running Cassandra) can be hot-swapped or be temporarily down without affecting the availability of the Cassandra cluster.
This is called elastic scalability <- TODO: No it's not! yeah, this kinda is->. Adding a new node and removing one will elasticly increase or decrease the cluster size without any major imapct on the distributed system.
All these features make Cassandra highly availabile and fault tolerance. Depending on the settings, a cluster of Cassandra nodes can be more or less consistent. From eventually consistent, the lowest setting for consistency, to highly consistent.
This can be referred to as tunable consistency and allows for a configuration dependent classification in the CAP Therorem. More on that in section \ref{sec:CassandraClusterArchitecture}.

Cassandra was originally developed by Facebook in 2008, the co-author of Amazons DynamoDB was involved
in the development process. Hence Cassandra shares similarities with the architecture of DynamoDB and in turn its inspiration Googles BigTable.
Now it is a free software project of the Apache Foundation. Development is mainly driven by DataStax.
A company designated for commercial Cassandra instances and enterprise support.


\section{Wide Column Store}

A wide column store is a tabular but not relational aproach to store data. It is not column oriented, since the rows are stored together. A row can have missing columns which are not stored on disk. This makes it sparse.
It can be thought like a key-value store where the value can have a subset of a predefined set of columns. Or, put in other words; "Sparse, distributed multi-dimensional sorted map" \cite{chang2008bigtable}
The naming can be explained as a key value store with wide (complex) values that consist of columns.


\section{Use-Cases Cassandra}
Cassandra comes with strong benefits over other database systemes. These benefits and extra features make it miss some features which one might expect as standard in databse systems.
It should be carefully considerd wether to use Cassandra or not since it is not the jack of all trades.
One benefit of Cassandr is fast writes which means it can handle a high throughput but the latency might not be too short. These writes include the operations of {INSERT}, {UPDATE} and {DELETE}. 
The way Cassandra handles those operations make them equal to a "regular" write operation. But more detail on that later on. Another key benefit is high availability. Due to its architecture and depending on the settings 
a Cassandra setup can be highly available. Since one of the base assumptions of the distributed system is the expensiveness of inter-node communication, it has an linear horizontal scalability. 
The communication between nodes does not increase with the size of node. Due to its distributed nature, the architecture does not rely on a master-slave setting and comes masterless. 
Fruthermore it can be configured to work with several clusters, globally distributed, wich makes for example multicloud Cassandra setups possible.
This means that any client can read from and write to any node. As described previously Cassandra is a wide column store database which results in having a flexible schema where rows can have missing columns that are not stored on disk.
Cassandra has an own query language which is similar SQL and called CQL (Cassandra Querry Language). This makes it easier to use Cassandra if knowledge from SQL is available.

If any of these points apply to a project or use-case Cassandra is probably a good fit.
But to be sure it is even more important to rule out the cases where Cassandra is for sure not a good fit.
The following paragraph will describe limits of Cassandra. If the use-case needs any of those features, Cassandra will most likely be not a good fit.
First of all single system instance with Cassandra should be avoided. Most of the features an benefits come with multiple node setups. Use-Cases which would need a dozen nodes seem to find a great fit with the distributed storage system.
Euqually important  Ever changing queries (The table is fine-tuned for pre defined queries)
        \item Lots of updates and deletes interspersed with reads (This slows Cassandra down)
        \item Transactions (ACID)
        \item Relations (joins, ...)
        \item Column aggregation ({GROUP BY}) \\
        (Being a wide cloumn oriented database, these functions are not available)
        \item {AUTO INCREMENT}
        \item Data validation (e.g. {NULL} constraint, uniqueness) (It has to be noted, that Cassandra does not read before writing. Any similar needs are not possible)
\end{itemize}

To sump it up there are general Use-Case conditions where Cassandra is a great fit.
Envornments with these needs should find a solution with Cassandra/
\begin{itemize}
    \item Large Deployments
    In order to make use of Cassandras features and benefits
    \item Tons of writes
      \begin{itemize}
        \item{"high performance at high write volumes with many concurrent client threads" \cite{cassandra_oreilly}}
      \end{itemize}
    \item Geographical distribution of data and database clients
    \item Different columns per row
\end{itemize}
{Ingenuity, architecture and featureset limited when used as single-node}
 {Several nodes? $\rightarrow$ might be a fit}
 {Dozen of nodes? $\rightarrow$ great fit}

{Consider read/write ratio}
    <2>[item]{As mentioned, cassandra optimized for write throughput}
    <2>[item]{Not many updates / data changes, slows read down}
    <2>[item]{Quote; you can even write to multiple nodes with multiple threads}

    <3>[item]{Globally deployed application, bring data to user}
    <3>[item]{Configure to replicate across multiple data centers}

    <4>[item]{Again: rows are sparse}


\section{Data Modelling in Cassandra}  % How to model data (or rather tables)



Data Modelling is different in Cassandra, compared to other Databases like traditional

\section{Using the Cassandra Query Language}  % How to use CQL
This section will give a short overview of how to interact with a Cassandra database using the Cassandra Query Language (CQL), which is mainly inspired by the Structured Query Language (SQL) \autocite{cqlAlexMeng, newInCQL3, cassandra3cqldocCreateKeystore}.

\subsection {Creating a keyspace}
This similarity start by looking into how the creation of a keyspace is performed:
\begin{verbatim}
/* Create a new keyspace in CQL */
CREATE KEYSPACE data WITH replication =
\{'class': 'SimpleStrategy', 'replication_factor': 3\};

/* Create a new database in SQL */
CREATE DATABASE data;
\end{verbatim}
Hereby the only difference is that instead of creating a Database a keyspace is created and it is possible to specify which replication parameters should be used. What these parameter meen and how they should be used is explained later in Section \ref{sec:CassandraClusterArchitecture} \autocite{cqlAlexMeng}.

\subsection{Creating a table}
After creating a keyspace a table has to be created in order to hold the data. As a database is always part of a keyspace it is either necessary to specify the keyspace in every query or to simple scope every subsequent query into a given keyspace by using the USE query \autocite{cassandra3cqldocUse}:
\begin{verbatim}
USE data;
\end{verbatim}

Using this keyspace a table can be created using the same syntax as in SQL \autocite{cqlAlexMeng, newInCQL3, cassandra3cqldocCreateTable}:
\begin{verbatim}
CREATE TABLE groups (
   group_name varchar,
   group_location varchar,
   added_date date,
   username varchar,
   PRIMARY KEY (...)
);
\end{verbatim}

Hereby the only difference is how the primary key can be specified:
\begin{figure}[ht]
    \centering
\begin{verbatim}
      partition key       clustering key  clustering key
       |       |                |            |
((groupname, group_location), added_date, username)
\end{verbatim}
    \caption{Parts of a primary key specification in CQL \autocite{cqlPrimaryKeyDefinition}}
    \label{fig:cassandra:primaryKeyDefinition}
\end{figure}
The first part of the definition will always be the partition key. If it is a compound of several columns they need to be marked by parenthesis separated by comma in order to state that they as a hole build the partition key. If necessary the primary key can be followed by several clustering keys. Keep in mind that the data will be ordered first by the first clustering key after that by the second and so on. This means that a order by has to first called on the first clustering key and the fine ordering can be done on the subsequent one. It will not be possible to only order by the second or other subsequent clustering keys when not ordering by the first \autocite{cqlPrimaryKeyDefinition, cassandra3cqldocCreateTable}.

\subsection{Interacting with data}
In order to manipulate cassandra only provides three possible methods \autocite{cassandra_paper}:
\begin{itemize}
    \item insert(table, key, rowMutation)
    \item get(table, key, columnName)
    \item delete(table, key, columnName)
\end{itemize}
All having in common that the whole primary key has to be specified in order to interact with the data. The only exception hereby is the getting of data where only the partition key has to be specified.

Important to note is that there is no interaction to update a data entry. The reason for that is that as Cassandra is optimized for high write throughput is is very costly to read any data before writing. This means that an update and insert known from SQL will perform the same action on the data \autocite{cqlAlexMeng, newInCQL3}:
\begin{verbatim}
/* Inserting Data */
INSERT INTO Person (lastname, name, email)
VALUES ('Musterfrau', 'Maxi', 'maxi@gmail.com');

/* Updating Data */
UPDATE Person SET email = 'maxi@gmail.com'
WHERE lastname='Musterfrau' AND name = 'MAXI';
\end{verbatim}

As getting and deleting data is also similar to SQL there is no need to go into it any further in this section \autocite{cqlAlexMeng, cassandra3cqldocSelect}:
\begin{verbatim}
/* Selecting Data */
SELECT * FROM Person
  WHERE lastname='Musterfrau' AND name = 'Maxi';
/* Deleting Data */
DELETE FROM Person
  WHERE lastname='Musterfrau' AND name = 'Maxi';
\end{verbatim}

\section{Local reads and writes}
In order to perform the requested changed to the data they have to be written into the database. This section will take a look into how the changes will be written on a single node not taken into account the cluster.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{img/cassandra_local_write.png}
    \caption{Writing data to Cassandra node \autocite{datastaxWriteData}}
    \label{fig:cassandra:writeData}
\end{figure}
In figure \ref{fig:cassandra:writeData} it can be seen that the write processe to cassadra involves three steps:
\begin{enumerate}
\item \textbf{Write to journal} Hereby the query is simple append to the journal on the disk, making it persistent even if the node goes down. As this action is a simple append it is very fast and leaves the data in a temporal order in the journal.
\item \textbf{Write to memtable} After writing to the journal the change is performed in the memtable putting the data into a Sorted String Table (SSTable). This form is the same form the data will be written on disk,
\item \textbf{Flush to disk when memtable is too big} This allows to simple flush the data and some metadata to the disk when it gets to big for the memory to hold it. Hereby a new data file is created not touching any of the previously written files, making this action also quite fast as no lookups have to be performed.
\end{enumerate}

After writing the data it also can be read again as shown by figure \ref{fig:cassandra:readData}:
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{img/cassandra_local_read.png}
    \caption{Reading data from Cassandra node \autocite{datastaxReadData}}
    \label{fig:cassandra:readData}
\end{figure}
\begin{enumerate}
    \item \textbf{Check caches} First the last query cache will be checked. Returning the data right away if it was requested in the near past.
    \item \textbf{Check memtable} Afterwards the memtable will be checked if it has recent activities on the requested data.
    \item \textbf{Find SSTable and location} If no entry was found in the memtable the data on disk will be checked by firstly determining in which memtable dump the dataset will be and then retriving it from there.
    \item \textbf{Merge with memtable} If it was necessary to retrieve the data from disk the data will be written to the memtable to allow later queries on the same data to succeed erlier.
\end{enumerate}

\section{Cluster Architecture}\label{sec:CassandraClusterArchitecture}  % How the cluster works
Which node has a certain piece of data is not determined by a master server. Any node can hash the partition key of an entry and arrive at a token. All nodes are placed somewhere on a \textit{token ring}, see figure \ref{fig:cassandra:tokenring}.

\newcommand{\Ray}{3cm}
\begin{figure}[ht]
  \begin{tikzpicture}
    % Nodes on the circles
    \node[circle,minimum width=2cm,minimum height=1cm,draw,name path=n1] (Node A) at (90:\Ray) {A};
    \node[circle,minimum width=2cm,minimum height=1cm,draw,name path=n2] (Node B) at (0:\Ray) {B};
    \node[circle,minimum width=2cm,minimum height=1cm,draw,name path=n3] (Node C) at (270:\Ray) {C};
    \node[circle,minimum width=2cm,minimum height=1cm,draw,name path=n4] (Node C) at (180:\Ray) {D};

    % Circle the nodes are placed on
    \path[name path=c] circle (\Ray);
    \path[name intersections={of=n1 and c,name=i1},
          name intersections={of=n2 and c,name=i2},
          name intersections={of=n3 and c,name=i3},
          name intersections={of=n4 and c,name=i4}
         ];

    % Arrows between the nodes
    \begin{scope}
      \pgfsetarrowsend{Stealth[scale=1.5]}

      \pgfpathmoveto{\pgfpointanchor{i1-2}{center}}
      \pgfpatharcto{\Ray}{\Ray}{0}{0}{0}{\pgfpointanchor{i2-1}{center}}
      \pgfusepath{draw}

      \pgfpathmoveto{\pgfpointanchor{i2-2}{center}}
      \pgfpatharcto{\Ray}{\Ray}{0}{0}{0}{\pgfpointanchor{i3-1}{center}}
      \pgfusepath{draw}

      \pgfpathmoveto{\pgfpointanchor{i3-2}{center}}
      \pgfpatharcto{\Ray}{\Ray}{0}{0}{0}{\pgfpointanchor{i4-2}{center}}
      \pgfusepath{draw}

      \pgfpathmoveto{\pgfpointanchor{i4-1}{center}}
      \pgfpatharcto{\Ray}{\Ray}{0}{0}{0}{\pgfpointanchor{i1-1}{center}}
      \pgfusepath{draw}
    \end{scope}

    % Labels on arrows
    \node[fill=white] at (45:\Ray) {0 to 63};
    \node[fill=white] at (135:\Ray) {64 to 127};
    \node[fill=white] at (225:\Ray) {128 to 191};
    \node[fill=white] at (315:\Ray) {192 to 255};
  \end{tikzpicture}
  \caption{Token Ring}
  \label{fig:cassandra:tokenring}
\end{figure}

The tokens from the node's location to the next node belong to it. That means if the hashing of a partition key results in a token between 0 and 63 the data will be written to or read from node A. Keep in mind: This doesn't however mean that this node is \textit{in control} of that data - when replication is configured all replicas are equal. The node with that token is just the starting point to determine the first of the replicas.

\paragraph{Replication} To ensure that all data continues to be available even if some nodes go down Cassandra replicates the data in the cluster. Replication can be defined for each keyspace when it is created. There is a simple strategy and a network topology aware strategy. \\
The \texttt{SimpleStrategy} places the $n$ replicas of a piece of data on the next $n-1$\footnote{The first node that holds the data also counts as a replica.} nodes located clockwise on the ring after the node with the token. \\
The \texttt{NetworkTopologyStrategy} strategy tries to be smart about the physical placement of replicas. It needs to be taught about in which datacenter and rack the nodes are located. To avoid losing data when an entire rack or datacenter fails this strategy prefers to spread it out.
It is recommened to use this strategy in production, unless all nodes are in a single rack, where both strategies are equivalent.  % TODO cite datastax

\paragraph{Rebalancing}

\begin{figure}[ht]
  \begin{tikzpicture}
    % Circle
    \fill[fill=black!25] (3, 0) arc (0:360:3);
    \node[circle,scale=2,fill=blue] at (0:3) {};
    \node[circle,scale=2,fill=blue] at (180:3) {};
    \node[circle,scale=2,fill=blue] at (200:3) {};
    \node[circle,scale=2,fill=blue] at (-20:3) {};

    \begin{scope}[shift={(9,0)}]
      \fill[fill=black!25] (3, 0) arc (0:360:3);
      \node[circle,scale=2,fill=blue] at (0:3) {};
      \node[circle,scale=2,fill=blue] at (180:3) {};
      \node[circle,scale=2,fill=green] at (270:3) {};
      \node[circle,scale=2,fill=green] at (90:3) {};
    \end{scope}
  \end{tikzpicture}
  \caption{Rebalancing}
  \label{fig:cassandra:rebalancing}
\end{figure}

TODO

\paragraph{Virtual Nodes}

TODO

\subsection{Distributed writes and reads (CAP Theorem)}

Cassandra has a masterless architecture; no single node controls any particular piece of data\footnote{Unless explicitly configured to do so}. This brings with it that a client can run query against any node of the cluster. In practice the client determines, either by some heuristic of proximity/latency or a round-robin algorithm, which node to use.
For one query that particular node becomes the \textit{coordinator node}; it coordinates the execution and distribution of that query.
First it hashes the partition key of the data to obtains the token and finds the node responsible for it. By using the replication strategy it can find out which replicas are responsible for that data as well. \\
When executing a read query the coordinator asks all replicas\footnote{A replica is every node responsible for that piece of data - the one determined by hashing as well as the others determined by replication strategy.} for the data. When more than \texttt{CL.read} replicas have answered the client is given an answer. When the replicas don't agree on the value of the data the client is sent the newest copy. Once all answers have come in the replicas with outdated information are sent a message on how to update theid data, this is called a \textit{read\_repair}. \\
When executing a write query the coordinator  TODO

Hinted Handoff: When the coordinator wants to write the data to one of the replicas which is currently unavailable

In short, the process looks like this:

\begin{enumerate}
  \item Client sends query to any node
  \item That node becomes coordinator
  \item Coordinator determines tokens by hashing
  \item Coordinator send write or read requests to all replicas determined by replication strategy
  \item For writing
    \begin{enumerate}
      \item Return success to client when more than \texttt{CL.write} nodes have acknowledged
      \item Write \textit{hinted handoff} to log for nodes that are currently unavailable
    \end{enumerate}
  \addtocounter{enumi}{-1}  % Both writing and reading should have the same number
  \item For reading
    \begin{enumerate}
      \item Return newest response when more than \texttt{CL.read} nodes have responed
      \item Send \texttt{read\_repair} to nodes with outdated data
    \end{enumerate}
\end{enumerate}

This process is also illustrated by figure~\ref{fig:cassandra:replication}.
The client queries node 4 which becomes the coordinator and then itself queries all replicas (1, 8 and 11).
When node 1 answers the default consistency level of \texttt{ONE} is satisfied and an answer is returned to the client.

\begin{figure}[ht]
  \begin{tikzpicture}
    % Circle
    \filldraw[fill=cyan] (\Ray, 0) arc (0:360:\Ray);

    % Nodes on the circle
    \foreach \x [count=\p] in {0,...,11} {
      \def\nodeColor{orange}
      \ifnum \p=1
        \def\nodeColor{green}
      \fi
      \ifnum \p=8
        \def\nodeColor{green}
      \fi
      \ifnum \p=11
        \def\nodeColor{green}
      \fi
      \ifnum \p=4
        \def\nodeColor{yellow}
      \fi
      \node[circle,draw,scale=0.75,fill=\nodeColor] (\p) at (90-\x*30:\Ray) {\p};
    };

    % Arrows between the nodes
    \draw [->,thick] (4) to[bend right=10] (1);
    \draw [<-,thick,draw=red] (4) to[bend left=10] (1);
    \draw [->,thick, dashed, blue] (4) -- (11);
    \draw [->,thick, dashed, blue] (4) -- (8);

    % Client plus its arrows
    \node[fill=cyan,draw,minimum width=1.5cm,minimum height=1cm,rounded corners=.1cm] (Client) at (2*\Ray, 0) {Client};
    \draw [->,thick,draw=red] (4) to[bend right=10] (Client);
    \draw [<-,thick] (4) to[bend left=10] (Client);

    % Legend
    \node[circle,draw,scale=0.75,fill=yellow,label=right:{Coordinator Node}] (coordinatorLegend) at (2.5*\Ray, 0.5) {N};
    \node[circle,draw,scale=0.75,fill=green,label=right:{Replica}] (replicaLegend) at (2.5*\Ray, -0.5) {N};
  \end{tikzpicture}
  \caption{Replication}
  \label{fig:cassandra:replication}
\end{figure}

\paragraph{Tuning Consistency} By default writes and reads need to be acknowledged by only a single replica. Usually data is configured to be replicated over multiple nodes. The result is that queries to Cassandra are highly available - any particular node can fail and the request will receive a sucessful response. This comes at the cost that not all replicas always have the same data - a lack of consistency. \\
Because different applications have different requirements of availability and consistency Cassandra offers several screws to turn to adjust it's alignment on that spectrum.

One of those is the consistency level. As previously mentioned in this section it determines how many nodes have to acknowledge a request until enough nodes have acknowledged completion.

Cassandra offers these, but not limited to these, options.

\begin{itemize}
  \item \texttt{ALL} replicas
  \item \texttt{QUORUM}: A majority of the replicas: half + 1
  \item \texttt{THREE} replicas
  \item \texttt{TWO} replicas
  \item \texttt{ONE} replicas (default)
  \item \texttt{ANY}: Only writing - At least one replica or a logged hinted handoff if all are unavailable
\end{itemize}

In addition to these levels Cassandra also offers others that also take into account how nodes are distributed into different datacenters. \\
For each query, but usually an entire client session, the consistency level can be chosen.

The \texttt{ALL} level yields the highest consistency. Only when all replicas have responded the response it given to the client. That means either all were updated or all were asked for their current dataset. That means there is no uncertainty about what the result is - all replicas must agree. The \texttt{ANY} consistency level is the

\paragraph{Replication Factor} Whenever creating a keyspace you have to configure its replication strategy. Whatever strategy you choose you have to determine how often you want a piece of data to be replicated. If you go with the lowest possible value of 1 the failure of any node will cause data loss (read and write failures). Increasing the replication factor means that more nodes can fail while requests can still be properly responded to - this means increased availability. It will, however, also mean that the additional replicas can get out of sync, which lowers the consistency.

We see that Cassandra by default Cassandra is highly available and only eventually consistent but it can be gradually tuned to being highly consistent and more susceptible to failure.

\section{Setup and Configuration}  % Setup
Cassandra is designed to be run in a cluster of multiple machines. That has to be kept in mind even when setting up a single instance. That single instance would form a single node cluster. This makes manual configuration unavoidable - every node needs to know how to join the cluster.
For the purpose of joining the cluster each node is configured with a list of \textit{seed nodes}. When starting up for the first time it asks those nodes about TODO. After joining the cluster is complete the distinction of \textit{seed nodes} is no longer relevant - all nodes are completely equal.

A simple but sufficient\footnote{The other settings can be left at their defaults.} configuration of the first node would look like shown in listing~\ref{lst:cassandra:first_node_config}.

\begin{listing}[ht]
  \begin{minted}{yaml}
# Open socket on this address
listen_address: "192.168.0.2"
# Tell other nodes its reachable on this address
broadcast_address: "3.14.1.59"

seed_provider:
  - class_name: org.apache.cassandra.locator.SimpleSeedProvider
    parameters:
      - seeds "3.14.1.59"

# Enable client communication
start_native_transport: true
  \end{minted}
  \caption{Configuration of first node}
  \label{lst:cassandra:first_node_config}
\end{listing}

For the masterless cluster to function all nodes need to be able to reach all other nodes. This is trivial if all are in the same subnet of the network. Then they can just reach each other by their IP addresses.
When they are in different subnets however they each have a private (local to their subnet) and public (inter subnet) address. This is very often the case in public cloud offerings. There are just not enough addresses available to give every node a public one.
Cassandra needs to know to things:
1. On what address to listen for oncoming TPC connections. That's what \texttt{listen\_address} is for. This is the local/private address.
2. What address to tell other nodes it's reachable under. This is set by \texttt{broadcast\_address} and it is the public address that all other nodes must be able to reach.
Therefore in a local network (with no address translation between the nodes) \texttt{listen\_address} and \texttt{broadcast\_address} are set to the same value.

In order to create a single cluster instance the node has to have its seed set to containing only its own address. Since it doesn't need to listen on a public or even private IP this should, in most cases, be \texttt{localhost}.
The first node of a cluster can be thought of as such a single instance node until others join.

In order for additional nodes to join the cluster configure them analogous to the first, changing the address values but keeping the seed list.
Upon starting them they will automatically communicate with the seed and join the cluster.

\section{Summary and Conclusion}
%- Summary, most important parts (results)
%- Limitations => Future research
%- All questions that came up ^
%- Research Question: Answer
%- Discussion
%- Conclusion

\cite{cassandra_paper}
